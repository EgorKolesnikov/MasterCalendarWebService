## CAP

Будем стремится к CP системе. Мы хотим, чтобы при добавлении или изменении какого-либо события
это изменение (добавление) было сразу же доступно всем. Мы можем пожертвовать недоступностью на запись,
но чтение должно быть доступно всегда (если хотя бы один узел в строю).

Но в нашем случае consistency будет рассматриваться без понятия linearizability. Т.е. нам не важно, в каком порядке
произошли изменения над двумя разными объектами, главное - чтобы они были сохранены.

Напомню, что система построена таким образом, что все события - общие. Создавать/удалять события может каждый,
также внесение изменений возможно над любым событием. В этом случае может возникнуть ряд проблем:

1) Регистрирование нового события возвращает его URI. Используя это URI два человека могут одновременно править данные
одного и того же события. В этом случае обновления будем рассматривать поочерёдно и использовать то, которое "пришло последним".
Так мы переносим ответственность на пользователя: когда он создал событие - только он один знает его URI.
Если он его кому-то передал, то он должен ожидать такого поведения.

2) Если один пользователь удалил событие, а второй пытается его изменить. В этом случае будем полагать, что удаление успешно, а на
обновление события возвращать 404.

В качестве бэкенда будем использовать MongoDB. В нашем случае она хороша для нас тем, что:
1) Встроена автоматическая [репликация](https://docs.mongodb.com/manual/replication/#replication-in-mongodb).
Нам нужно, чтобы пользователь всегда ходил в мастера по умолчанию (и на чтение и на запись). Так как мы позволяем системе быть 
недоступной на запись, то смена мастера нам не вредит - мы просто запрещаем запись и позволяем только чтение (со slave-ов)
2) "Дружит" с JSON-ом. У нас вся выдача и все сообщения происходят в JSON формате.


#### Little optimizations
1) Операций чтения ожидается гораздо больше чем операций записи. Для этого можно по умолчанию чтения направлять сразу в slave
реплики, чтоы снизить нагрузку и немного улучшить производительность мастера на запросах записи.
2) По идее, событие наиболее популярно за некоторый небольшой промежуток до его начала (все начинают смотреть, что за событие,
его описание, время проведения и т.д.). В этом случае полезно, например, кэшировать запросы к базе, и/или использовать процесс,
который периодически будет доставать "ближайшие события" и обновлять кэш.

Эти оптимизации повлияют на нашу систему с точки зрения CAP следующим образом:
1) В первом случае возможна ситуация, когда кто-то создал событие, оно сохранилось в мастере, но не успело доехать до реплик, когда уже кто-то пытается его выгрузить. Этот лаг, конечно, не должен быть слишком большим, но всё равно он есть и мы вынуждены жить с eventual consistency.
2) Так как мы не знаем, куда балансер распределит запрос очередного пользователя, то данные кэши необходимо поддерживать на каждом инстансе (и не важно насколько лёгкий этот кэш: или массив в памяти, или настоящий прокси сервер). Событие может быть создано уже после того, как кэш обновился, из-за чего у нас возникает необходимость следить ещё и за консистентностью кэша.
